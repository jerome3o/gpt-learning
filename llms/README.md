# Open Source LLMs

This is a list of open source LLMs, some resources and information on each.

I plan to try and run all of these on the ML rig.

# Feasible

## Dolly 2.0

Fine tuned ✔️

Parameters: 3B, 6B, 7B, 12B
Creator: [DataBricks](https://www.databricks.com/)
Model Family: [pythia](https://github.com/EleutherAI/pythia)
Fine Tuning Dataset: [databricks-dolly-15k](https://github.com/databrickslabs/dolly/tree/master/data)

* [Analytics India Mag](https://analyticsindiamag.com/databricks-dolly-2-0-is-a-game-changer-in-the-open-source-llms/)
* [Hugging Face](https://huggingface.co/databricks)


## Flan-T5

* [Hugging Face](https://huggingface.co/google/flan-t5-xl)
* [Article](https://exemplary.ai/blog/flan-t5)

## Open Assistant

Fine tuned ✔️

Based on Pythia models

Parameters: 1.4B, 6.9B, 12B

* [Website](https://open-assistant.io/)
* [GitHub](https://github.com/LAION-AI/Open-Assistant)
* [Hugging Face](https://huggingface.co/OpenAssistant)

## BLOOM (only the smaller ones)

Not fine tuned

Parameters: 560M, 1.1B, 1.7B, 3B, 7.1B, 176B (!!!)
Creator: [BigScience](https://bigscience.huggingface.co/)

* [Overview](https://huggingface.co/docs/transformers/model_doc/bloom)
* [Hugging Face (7.1B)](https://huggingface.co/bigscience/bloom-7b1/tree/main)


## GPT-Neo

Not fine tuned

Parameters: 125M, 1.3B, 2.7B
Creator: [EleutherAI](https://www.eleuther.ai/)

* [GitHub](https://github.com/EleutherAI/gpt-neo)
* [Hugging Face 2.7B](https://huggingface.co/EleutherAI/gpt-neo-2.7B)

## GPT-NeoX

Not fine tuned

Parameters: 20B
Creator: [EleutherAI](https://www.eleuther.ai/)

* [GitHub](https://github.com/EleutherAI/gpt-neox)
* [Hugging Face](https://huggingface.co/EleutherAI/gpt-neox-20b)

## GPT-J

Not fine tuned

Parameters: 6B
Creator: [EleutherAI](https://www.eleuther.ai/)

* [HuggingFace](https://huggingface.co/EleutherAI/gpt-j-6b)

## Pythia

Not fine tuned

Creator: [EleutherAI](https://www.eleuther.ai/)

* [GitHub](https://github.com/EleutherAI/pythia)


## CerebrasGPT

Not fine tuned

* [Article](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/)

## LLaMA

Meta's LLM architecture + research only trained models - not for commercial use. I don't think it's fine tuned.

* [article](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)

## Alpaca

Stanford using the LLaMa, fined tuned, not for commercial use.

* [GitHub](https://github.com/tatsu-lab/stanford_alpaca)

# Too Big

## GLM-130B

Chinese and English pre-trained model

* [GitHub](https://github.com/THUDM/GLM-130B)
