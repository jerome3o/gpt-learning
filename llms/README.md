# Open Source LLMs

This is a list of open source LLMs, some resources and information on each.

I plan to try and run all of these on the ML rig.

# Feasible

## Dolly 2.0

Parameters: 12, 7, 6, 3 (B)
Creator: [DataBricks](https://www.databricks.com/)
Model Family: [pythia](https://github.com/EleutherAI/pythia)
Fine Tuning Dataset: [databricks-dolly-15k](https://github.com/databrickslabs/dolly/tree/master/data)

* [Analytics India Mag](https://analyticsindiamag.com/databricks-dolly-2-0-is-a-game-changer-in-the-open-source-llms/)
* [Hugging Face](https://huggingface.co/databricks)

## BLOOM (only the smaller ones)

Parameters: 560M, 1.1B, 1.7B, 3B, 7.1B, 176B (!!!)
Creator: [BigScience](https://bigscience.huggingface.co/)

* [Overview](https://huggingface.co/docs/transformers/model_doc/bloom)
* [Hugging Face (7.1B)](https://huggingface.co/bigscience/bloom-7b1/tree/main)


## GPT-Neo

Not fine tuned

Parameters: 125M, 1.3B, 2.7B
Creator: [EleutherAI](https://www.eleuther.ai/)

* [GitHub](https://github.com/EleutherAI/gpt-neo)
* [Hugging Face 2.7B](https://huggingface.co/EleutherAI/gpt-neo-2.7B)

## GPT-NeoX

Not fine tuned

Parameters: 20B
Creator: [EleutherAI](https://www.eleuther.ai/)

* [GitHub](https://github.com/EleutherAI/gpt-neox)
* [Hugging Face](https://huggingface.co/EleutherAI/gpt-neox-20b)

## GPT-J

Not fine tuned

Parameters: 6B
Creator: [EleutherAI](https://www.eleuther.ai/)

* [HuggingFace](https://huggingface.co/EleutherAI/gpt-j-6b)

## Pythia

Creator: [EleutherAI](https://www.eleuther.ai/)

* [GitHub](https://github.com/EleutherAI/pythia)


## GPT-2

## PaLM

## OPT

## CerebrasGPT

## LLaMA

## Alpaca

# Too Big

## GLM-130B

Chinese and English pre-trained model

* [GitHub](https://github.com/THUDM/GLM-130B)
