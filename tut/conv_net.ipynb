{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4aaa92-fc07-4527-a47a-c7c7cfee3686",
   "metadata": {},
   "source": [
    "My code along with https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f14c2-f38a-4302-a080-51cfbd5274c1",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9cf931-0308-45d4-ba35-8183d4a31cb4",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9c948bd-7e9e-4682-900b-98e0a928824c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d273c8c-67be-4a79-ba2f-f01f1e516958",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from helpers.stats import model_mem_size, mem_size, mem_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b301a5-497b-4c5e-8fe2-ff386c5d205e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Have a look at some images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e3196-f8e4-4194-ab70-bce52fb092c6",
   "metadata": {},
   "source": [
    "# Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aca8f4f5-2d0e-44b7-af60-32147564ffa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# define trainload and testload?\n",
    "\n",
    "_batch_size = 2000\n",
    "\n",
    "tf = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "        (0.5, 0.5, 0.5),\n",
    "        (0.5, 0.5, 0.5),\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    \"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=tf,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    \"./data\",\n",
    "    train=False,\n",
    "    transform=tf,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    _batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "# Not sure where these classes and their order actually came from\n",
    "classes = (\n",
    "    'plane',\n",
    "    'car',\n",
    "    'bird',\n",
    "    'cat',\n",
    "    'deer',\n",
    "    'dog',\n",
    "    'frog',\n",
    "    'horse',\n",
    "    'ship',\n",
    "    'truck',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0450950a-c1e4-4a8b-87ec-4e9695512076",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc7828eb-a4ff-49ba-bb35-d5f59df8cf06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "_N_IMAGE_CHANNELS = 3\n",
    "\n",
    "\n",
    "class ConvStack(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        out_channels: List[int] = None,\n",
    "        kernel_sizes: List[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._out_channels = out_channels or [50, 50]\n",
    "        self._kernel_sizes = kernel_sizes or [5, 5]\n",
    "        \n",
    "        # # not including max pool kernel as we're dealing with tiny images\n",
    "        \n",
    "        # self._pool_kernel = 1\n",
    "        # self._pool_stride = 1\n",
    "        \n",
    "        # self.pool = nn.MaxPool2d(\n",
    "        #     kernel_size=self._pool_kernel,\n",
    "        #     stride=self._pool_stride,\n",
    "        # )\n",
    "        \n",
    "        modules = [\n",
    "            nn.Conv2d(\n",
    "                in_channels=_N_IMAGE_CHANNELS,\n",
    "                out_channels=self._out_channels[0],\n",
    "                kernel_size=self._kernel_sizes[0],\n",
    "            ),\n",
    "            # self.pool,\n",
    "        ]\n",
    "        \n",
    "        for i in range(1, len(self._out_channels)):\n",
    "            modules.extend([\n",
    "                nn.Conv2d(\n",
    "                    in_channels=self._out_channels[i - 1],\n",
    "                    out_channels=self._out_channels[i],\n",
    "                    kernel_size=self._kernel_sizes[i],\n",
    "                ),\n",
    "                # self.pool,\n",
    "            ])\n",
    "        self.sequential = nn.Sequential(*modules)\n",
    "        \n",
    "        \n",
    "    def get_output_size(self, image_size: int):\n",
    "        # note that it's x for x in .. as opposed to x-1 due to the max pooling\n",
    "        # converted back to x-1 as I'm no longer using max pooling\n",
    "        output_size = image_size - sum(x - 1 for x in self._kernel_sizes)\n",
    "        return output_size * output_size * self._out_channels[-1]\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.sequential(x)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, image_size: int):\n",
    "        super().__init__()\n",
    "        self._image_size = image_size\n",
    "        self.conv_stack = ConvStack(\n",
    "            out_channels=[1000, 600, 300, 100],\n",
    "            kernel_sizes=[8, 7, 6, 5],\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(\n",
    "            self.conv_stack.get_output_size(self._image_size),\n",
    "            10_000,\n",
    "        )\n",
    "        self.fc2 = nn.Linear(10_000, 10_000)\n",
    "        self.fc3 = nn.Linear(10_000, len(classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_stack(x)\n",
    "        # import ipdb\n",
    "        # ipdb.set_trace()\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89ac358-8b9d-4ec9-8ade-980efb35ac67",
   "metadata": {},
   "source": [
    "## Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50f4d372-0bd6-494f-8a2e-c1c524a7834b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "import time\n",
    "\n",
    "@contextmanager\n",
    "def _log(message: str, min_time=None):\n",
    "    t_start = time.time()\n",
    "    if min_time is None:\n",
    "        print(f\"{message}\")\n",
    "    yield\n",
    "    duration = time.time() - t_start\n",
    "    if min_time is None or min_time < duration:\n",
    "        mem_summary()\n",
    "        print(f\"{message} complete ({duration:0.2f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a2267d-aac1-4f07-96dc-4bd652dff383",
   "metadata": {},
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a51324-da68-468a-8cdb-8f8eccf5a4d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "903.9MiB\n",
      "cuda:0: 926.0MiB (5.66%)\n",
      "cuda:1: 0.0B (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [05:21<00:00, 12.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, duration: 321.3566553592682, loss:  2.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [05:21<00:00, 12.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, duration: 321.06624579429626, loss:  2.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [05:21<00:00, 12.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, duration: 321.2367694377899, loss:  2.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 28%|██▊       | 7/25 [01:30<03:51, 12.84s/it]"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "min_log_time = 20\n",
    "lr = 0.001\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# define net\n",
    "net = Net(image_size=32)\n",
    "net.to(device)\n",
    "\n",
    "model_mem_size(net)\n",
    "mem_summary()\n",
    "\n",
    "# define optimiser\n",
    "optimiser = optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "# define loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "epoch = 0\n",
    "n_epochs = 20\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    t_start = time.time()\n",
    "    cumulative_loss = 0\n",
    "    for i, (images, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        \n",
    "        with _log(f\"loading data to {device}\", min_time=min_log_time):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "        with _log(\"making predictions\", min_time=min_log_time):\n",
    "            # make predictions\n",
    "            y_pred = net(images)\n",
    "\n",
    "        with _log(\"calculating loss\", min_time=min_log_time):\n",
    "            # calculate loss\n",
    "            loss = loss_function(y_pred, labels)\n",
    "\n",
    "        with _log(\"zeroing grad\", min_time=min_log_time):\n",
    "            # zero gradients\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "        with _log(\"running backward pass\", min_time=min_log_time):\n",
    "            # calculate gradients from loss\n",
    "            loss.backward()\n",
    "\n",
    "        with _log(\"stepping optimiser\", min_time=min_log_time):\n",
    "            # step the optim\n",
    "            optimiser.step()\n",
    "\n",
    "        # print(f\"epoch {epoch} iter {i}: {loss}\")\n",
    "        cumulative_loss += loss.item()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    duration = time.time() - t_start\n",
    "    print(f\"epoch: {epoch}, duration: {duration}, loss: {cumulative_loss / (i + 1): 0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0cbc87-1a86-48d9-95c8-02eabe7acf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cd1d20-d854-4c18-ac83-a1eb260d1111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = None\n",
    "net = None\n",
    "optimiser = None\n",
    "cumulative_loss = None\n",
    "loss = None\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.ipc_collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "7d97c8ac-06ea-4258-b9ec-7691a562d0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Accuracy:\n",
    "    total: int\n",
    "    correct: int\n",
    "    # TODO: per class\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.correct/self.total*100:0.3f}%\"\n",
    "\n",
    "\n",
    "def _calc_accuracy(\n",
    "    model: Net, \n",
    "    dataset: torch.utils.data.DataLoader,\n",
    ") -> Accuracy:\n",
    "    # test accuracy\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for images, labels in dataset:\n",
    "            images = images.to(device)\n",
    "            output = model(images)\n",
    "            predictions = torch.max(output, 1)[1].to(\"cpu\")\n",
    "            total += labels.size(0)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "\n",
    "    return Accuracy(\n",
    "        total=total,\n",
    "        correct=correct,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "d52343ca-3a4b-4ab2-8993-4c351f59da01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.468%\n",
      "54.510%\n"
     ]
    }
   ],
   "source": [
    "print(_calc_accuracy(net, train_loader))\n",
    "print(_calc_accuracy(net, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de649ad-59fe-4be2-b4ff-de5422494a1a",
   "metadata": {},
   "source": [
    "TODO: figure out how to effectively scale a conv nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9455e169-cdb8-434e-aa75-b5a5ccf31fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 3, 5, 5])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv1.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "bb35d33f-c1be-4248-9f09-9ec4e873a048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conv_out = net.conv1(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "33df4dfc-108d-4058-b9e3-6408e1b3ffbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 3, 32, 32])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9df7a564-1c81-4f8c-ad04-ace438b56b08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 50, 28, 28])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3518b6f0-4926-4cce-b63f-91cdf1763676",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 50, 14, 14])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool_out = net.pool(conv_out)\n",
    "pool_out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2de9a5eb-ddc4-44ca-9fee-216f23da174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_out_2 = F.relu(net.conv2(pool_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a30acd20-a73e-4bd4-b10b-014b8ea61721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pool_out_2 = net.pool(conv_out_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e97e9d7c-043c-42fa-a999-164dfdf6cfe8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 16, 5, 5])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool_out_2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eed61e9-a927-4bed-968e-48f8fd811ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in trainloader:\n",
    "    output = net(images)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    mask = predicted == labels\n",
    "    correct += mask.sum().item()\n",
    "    total += mask.size(0)\n",
    "\n",
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
