{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following [this tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7a3f7fdf70>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 3\n",
    "hidden_size = 3\n",
    "sequence_length = 5\n",
    "\n",
    "lstm = nn.LSTM(embedding_dim, hidden_size)\n",
    "inputs = [torch.randn(1, embedding_dim) for _ in range(sequence_length)]\n",
    "\n",
    "hidden = (torch.randn(1, 1, embedding_dim), torch.randn(1, 1, embedding_dim))\n",
    "\n",
    "\n",
    "for i in inputs:\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2490, -0.0525,  0.3253]],\n",
      "\n",
      "        [[ 0.1655, -0.0304,  0.3348]],\n",
      "\n",
      "        [[-0.1104, -0.1085,  0.7568]],\n",
      "\n",
      "        [[-0.0148, -0.0855,  0.4162]],\n",
      "\n",
      "        [[ 0.0703, -0.1089,  0.2071]]], grad_fn=<MkldnnRnnLayerBackward0>)\n",
      "tensor([[[ 0.0703, -0.1089,  0.2071]]], grad_fn=<StackBackward0>)\n",
      "tensor([[[ 0.2099, -0.3541,  0.9947]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
    "out, (h_n, c_n) = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(h_n)\n",
    "print(c_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hidden\u001b[39m.\u001b[39;49msize()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "hidden.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for Part-of-Speech Tagging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yoinked the UniversalDependencies training data set from [here](https://github.com/UniversalDependencies/UD_English-GUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing data\n",
    "\n",
    "from typing import List, Tuple, Callable, Dict\n",
    "import conllu\n",
    "\n",
    "\n",
    "_training_file = \"./data/en_gum-ud-train.conllu\"\n",
    "_testing_file = \"./data/en_gum-ud-test.conllu\"\n",
    "\n",
    "\n",
    "def _load_data(f: str) -> List[Tuple[List[str], List[str]]]:\n",
    "    with open(f) as f:\n",
    "        data = conllu.parse(f.read())\n",
    "\n",
    "    return [\n",
    "        list(zip(\n",
    "            *[(t[\"form\"], t[\"upos\"])\n",
    "            for t in token_list]\n",
    "        ))\n",
    "        for token_list in data\n",
    "    ]\n",
    "\n",
    "training_data = _load_data(_training_file)\n",
    "testing_data = _load_data(_testing_file)\n",
    "\n",
    "# define word_to_idx, tag_to_idx\n",
    "\n",
    "# very memory inefficient, however it's ok because we're only \n",
    "# dealing with a little bit of data\n",
    "vocab = set(\n",
    "    [x for words, _ in training_data for x in words]\n",
    "    + [x for words, _ in testing_data for x in words]\n",
    ")\n",
    "tags = set(\n",
    "    [x for _, tags in training_data for x in tags]\n",
    "    + [x for _, tags in testing_data for x in tags]\n",
    ")\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "tag_count = len(tags)\n",
    "\n",
    "word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
    "tag_to_idx = {t: i for i, t in enumerate(tags)}\n",
    "\n",
    "\n",
    "def prep_sequence(sent: List[str], to_idx: Callable[[str], int]):\n",
    "    return torch.tensor(list(map(to_idx.get, sent)), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmPosTagger(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        hidden_size: int,\n",
    "        vocab_size: int,\n",
    "        tag_count: int,\n",
    "        num_layers: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._hidden_size = hidden_size\n",
    "        self._vocab_size = vocab_size\n",
    "        self._tag_count = tag_count\n",
    "        self._num_layers = num_layers\n",
    "\n",
    "        self.emb = nn.Embedding(\n",
    "            num_embeddings=self._vocab_size,\n",
    "            embedding_dim=self._embedding_dim,\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self._embedding_dim,\n",
    "            hidden_size=self._hidden_size,\n",
    "            num_layers=self._num_layers,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=self._hidden_size,\n",
    "            out_features=self._tag_count,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,  # (seq_length, batch_size, 1)\n",
    "    ):\n",
    "        x = self.emb(x)  # (seq_length, batch_size, emb_dim)\n",
    "        all_hidden_states, (_final_hidden_state, _final_cell_state) = self.lstm(x)\n",
    "        # all_hidden_states (seq_length, batch_size, hidden_size)\n",
    "\n",
    "        output = self.fc(all_hidden_states)  # (seq_length, batch_size, tag_count)\n",
    "        return output\n",
    "\n",
    "\n",
    "def calculate_accuracy(\n",
    "    model: LstmPosTagger,\n",
    "    data: List[Tuple[List[str], List[str]]],\n",
    "    word_to_idx: Dict[str, int],\n",
    "    tag_to_idx: Dict[str, int],\n",
    ") -> float:\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x_raw, tags_raw in data:\n",
    "            x = prep_sequence(x_raw, word_to_idx)\n",
    "            tags = prep_sequence(tags_raw, tag_to_idx)\n",
    "            output = model(x)\n",
    "            logits = F.softmax(output, dim=-1)\n",
    "            _, tags_pred = torch.max(logits, dim=-1)\n",
    "            correct += (tags_pred == tags).sum().item()\n",
    "            total += x.size(0)\n",
    "    return correct / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 30\n",
    "HIDDEN_DIM = 40\n",
    "LEARNING_RATE = 0.01\n",
    "N_EPOCHS = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss:  2.25, test acc: 0.38, train acc: 0.37\n",
      "epoch: 1, loss:  1.77, test acc: 0.51, train acc: 0.51\n",
      "epoch: 2, loss:  1.50, test acc: 0.56, train acc: 0.56\n",
      "epoch: 3, loss:  1.33, test acc: 0.59, train acc: 0.60\n",
      "epoch: 4, loss:  1.21, test acc: 0.62, train acc: 0.62\n",
      "epoch: 5, loss:  1.12, test acc: 0.63, train acc: 0.65\n",
      "epoch: 6, loss:  1.06, test acc: 0.65, train acc: 0.66\n",
      "epoch: 7, loss:  1.00, test acc: 0.66, train acc: 0.68\n",
      "epoch: 8, loss:  0.96, test acc: 0.67, train acc: 0.69\n",
      "epoch: 9, loss:  0.93, test acc: 0.68, train acc: 0.70\n",
      "epoch: 10, loss:  0.90, test acc: 0.69, train acc: 0.70\n",
      "epoch: 11, loss:  0.87, test acc: 0.69, train acc: 0.71\n",
      "epoch: 12, loss:  0.84, test acc: 0.70, train acc: 0.71\n",
      "epoch: 13, loss:  0.82, test acc: 0.70, train acc: 0.72\n",
      "epoch: 14, loss:  0.80, test acc: 0.70, train acc: 0.72\n",
      "epoch: 15, loss:  0.78, test acc: 0.71, train acc: 0.73\n",
      "epoch: 16, loss:  0.77, test acc: 0.71, train acc: 0.73\n",
      "epoch: 17, loss:  0.75, test acc: 0.71, train acc: 0.74\n",
      "epoch: 18, loss:  0.74, test acc: 0.72, train acc: 0.74\n",
      "epoch: 19, loss:  0.72, test acc: 0.72, train acc: 0.74\n",
      "epoch: 20, loss:  0.71, test acc: 0.72, train acc: 0.75\n",
      "epoch: 21, loss:  0.70, test acc: 0.72, train acc: 0.75\n",
      "epoch: 22, loss:  0.69, test acc: 0.73, train acc: 0.75\n",
      "epoch: 23, loss:  0.67, test acc: 0.73, train acc: 0.76\n",
      "epoch: 24, loss:  0.66, test acc: 0.73, train acc: 0.76\n"
     ]
    }
   ],
   "source": [
    "model = LstmPosTagger(\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_size=HIDDEN_DIM,\n",
    "    vocab_size=vocab_size,\n",
    "    tag_count=tag_count,\n",
    ")\n",
    "loss_function = F.cross_entropy\n",
    "optimiser = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "n_training_points = len(training_data)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    cum_loss = 0\n",
    "    for x_raw, tags_raw in training_data:\n",
    "        x = prep_sequence(x_raw, word_to_idx)\n",
    "        tags = prep_sequence(tags_raw, tag_to_idx)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        tags_pred = model(x)\n",
    "        loss = loss_function(tags_pred, tags)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        cum_loss += loss.item()\n",
    "\n",
    "    train_acc = calculate_accuracy(\n",
    "        model,\n",
    "        training_data,\n",
    "        word_to_idx,\n",
    "        tag_to_idx,\n",
    "    )\n",
    "    test_acc = calculate_accuracy(\n",
    "        model,\n",
    "        testing_data,\n",
    "        word_to_idx,\n",
    "        tag_to_idx,\n",
    "    )\n",
    "    print(\n",
    "        f\"epoch: {epoch}, \"\n",
    "        f\"loss: {cum_loss/n_training_points: 0.2f}, \"\n",
    "        f\"test acc: {test_acc:0.2f}, \"\n",
    "        f\"train acc: {train_acc:0.2f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
