{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3a6cb3-d272-44ec-a666-ed634b9486f1",
   "metadata": {},
   "source": [
    "# Building a Deep Neural Net for Sentiment Analysis on IMDb Reviews\n",
    "\n",
    "## 1. Data collection and preprocessing\n",
    "- Collect a dataset of IMDb reviews\n",
    "- Preprocess the text data (tokenization, lowercasing, removing special characters, etc.)\n",
    "- Split the dataset into training, validation, and test sets\n",
    "\n",
    "## 2. **Model selection and architecture**\n",
    "- Research different types of deep learning models (**RNN**, LSTM, GRU, CNN, Transformer)\n",
    "- Decide on a model architecture\n",
    "- Experiment with pre-trained models (BERT, GPT, RoBERTa) for fine-tuning\n",
    "\n",
    "## 3. Model training and hyperparameter tuning\n",
    "- Set up a training loop\n",
    "- Use backpropagation to update the model's weights based on the loss function\n",
    "- Experiment with different hyperparameters (learning rate, batch size, dropout rate, etc.) and optimization algorithms (Adam, RMSprop, etc.)\n",
    "- Monitor performance on the validation set during training\n",
    "\n",
    "## 4. Model evaluation and refinement\n",
    "- Evaluate the model on the test set using relevant metrics (accuracy, F1 score, precision, recall, etc.)\n",
    "- Identify areas for improvement and iterate on the model architecture, training process, or preprocessing techniques\n",
    "\n",
    "## 5. \"Extra for experts\" ideas\n",
    "- Handle class imbalance (oversampling, undersampling, or SMOTE)\n",
    "- Experiment with different word embeddings (Word2Vec, GloVe, FastText) or contextual embeddings (ELMo, BERT)\n",
    "- Explore advanced model architectures (multi-head attention, capsule networks, memory-augmented networks)\n",
    "- Investigate transfer learning or multi-task learning\n",
    "- Conduct error analysis to understand and address specific issues\n",
    "- Develop a user interface or API for your sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4e9b2ba-e5d6-453c-aea1-d445c4249e61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        emb_dim: int = 300,\n",
    "        hidden_size: int = 400,\n",
    "        n_rnn_layers: int = 5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=emb_dim,\n",
    "        )\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_rnn_layers,\n",
    "        )\n",
    "\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 2),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor):\n",
    "        # x shape: (B, L)\n",
    "        # convert token indices to embedding values\n",
    "        x = self.emb(x)\n",
    "        # x shape: (B, L, Emb dim)\n",
    "\n",
    "        x = x.transpose(0, 1)\n",
    "        # x shape: (L, B, Emb dim)\n",
    "\n",
    "        # pack the sequence\n",
    "        x = pack_padded_sequence(x, lengths, enforce_sorted=False)\n",
    "\n",
    "        # run the rnn, only taking the final rnn hidden state from the last layer\n",
    "        # TODO: understand the difference between the two outputs more\n",
    "        _, x = self.rnn(x)\n",
    "        # x shape: (B, n_rnn_layers, Hidden size?)\n",
    "        \n",
    "        # take only the last layer\n",
    "        x = x[-1, :, :]\n",
    "        # x shape: (B, Hidden size?)\n",
    "        \n",
    "        return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37b1b78a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import Dataset\n",
    "\n",
    "# load in tokenized data\n",
    "data_dict = torch.load(\"data/imdb_data.pt\")\n",
    "data = data_dict[\"reviews\"]\n",
    "labels = data_dict[\"labels\"]\n",
    "lengths = data_dict[\"lengths\"]\n",
    "\n",
    "# split into train and test by 80:20\n",
    "train_data = data[:int(len(data) * 0.8)]\n",
    "train_labels = labels[:int(len(data) * 0.8)]\n",
    "train_lengths = lengths[:int(len(data) * 0.8)]\n",
    "\n",
    "test_data = data[int(len(data) * 0.8):]\n",
    "test_labels = labels[int(len(data) * 0.8):]\n",
    "test_lengths = lengths[int(len(data) * 0.8):]\n",
    "\n",
    "\n",
    "# load in tokenizer\n",
    "tokenizer = tokenizers.Tokenizer.from_file(\"models/tokenizer.json\")\n",
    "vocab_size = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885a232e",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb64173b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:10<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATE - Epoch: 0, Loss: 27.722915649414062, Accuracy:  50.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:02<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.38496732711792, Accuracy:  50.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:02<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.384955883026123, Accuracy:  50.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:02<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 1.3849314451217651, Accuracy:  50.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:02<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 1.3848965167999268, Accuracy:  50.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 36/79 [00:28<00:33,  1.29it/s]"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# create the model\n",
    "model = RNNModel(vocab_size=vocab_size)\n",
    "model = model.to(device)\n",
    "\n",
    "train_data = train_data.to(device)\n",
    "train_labels = train_labels.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "eval_interval = 5\n",
    "\n",
    "acc_best = 0\n",
    "\n",
    "# train the model\n",
    "for epoch in range(500):\n",
    "    if epoch % eval_interval == 0:\n",
    "        # calculate overall loss (need batching for memory reasons)\n",
    "        loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, len(train_data), 2*batch_size)):\n",
    "                training_data_batch = train_data[i:i+batch_size]\n",
    "                training_labels_batch = train_labels[i:i+batch_size]\n",
    "                training_lengths_batch = train_lengths[i:i+batch_size]\n",
    "\n",
    "                output = model(training_data_batch, lengths=training_lengths_batch)\n",
    "                ## Calculate correct predictions\n",
    "                _, y_predicted = torch.max(output, 1)\n",
    "                _, y = torch.max(training_labels_batch, 1)\n",
    "                total += training_labels_batch.size(0)\n",
    "                correct += (y_predicted == y).sum().item()\n",
    "                loss += nn.functional.cross_entropy(output, training_labels_batch)\n",
    "        \n",
    "        torch.save(model.state_dict(), \"models/rnn_latest.pt\")\n",
    "        acc = correct/total*100\n",
    "        if acc > acc_best:\n",
    "            torch.save(model.state_dict(), \"models/rnn_best.pt\")\n",
    "            \n",
    "        print(f\"UPDATE - Epoch: {epoch}, Loss: {loss.item()}, Accuracy: {acc: .2f}\")\n",
    "        \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    cumulative_loss = 0\n",
    "    \n",
    "    for i in tqdm(range(0, len(train_data), batch_size)):\n",
    "        training_data_batch = train_data[i:i+batch_size]\n",
    "        training_labels_batch = train_labels[i:i+batch_size]\n",
    "        training_lengths_batch = train_lengths[i:i+batch_size]\n",
    "\n",
    "        output = model(training_data_batch, lengths=training_lengths_batch)\n",
    "\n",
    "        # forward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss = nn.functional.cross_entropy(output, training_labels_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ## Calculate correct predictions\n",
    "        _, y_predicted = torch.max(output, 1)\n",
    "        _, y = torch.max(training_labels_batch, 1)\n",
    "        total += training_labels_batch.size(0)\n",
    "        correct += (y_predicted == y).sum().item()\n",
    "        cumulative_loss += nn.functional.cross_entropy(output, training_labels_batch)\n",
    "    \n",
    "    acc = correct/total*100\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss.item()}, Accuracy: {acc: .2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03453a24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
