{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3a6cb3-d272-44ec-a666-ed634b9486f1",
   "metadata": {},
   "source": [
    "# Building a Deep Neural Net for Sentiment Analysis on IMDb Reviews\n",
    "\n",
    "## 1. Data collection and preprocessing\n",
    "- Collect a dataset of IMDb reviews\n",
    "- Preprocess the text data (tokenization, lowercasing, removing special characters, etc.)\n",
    "- Split the dataset into training, validation, and test sets\n",
    "\n",
    "## 2. **Model selection and architecture**\n",
    "- Research different types of deep learning models (**RNN**, LSTM, GRU, CNN, Transformer)\n",
    "- Decide on a model architecture\n",
    "- Experiment with pre-trained models (BERT, GPT, RoBERTa) for fine-tuning\n",
    "\n",
    "## 3. Model training and hyperparameter tuning\n",
    "- Set up a training loop\n",
    "- Use backpropagation to update the model's weights based on the loss function\n",
    "- Experiment with different hyperparameters (learning rate, batch size, dropout rate, etc.) and optimization algorithms (Adam, RMSprop, etc.)\n",
    "- Monitor performance on the validation set during training\n",
    "\n",
    "## 4. Model evaluation and refinement\n",
    "- Evaluate the model on the test set using relevant metrics (accuracy, F1 score, precision, recall, etc.)\n",
    "- Identify areas for improvement and iterate on the model architecture, training process, or preprocessing techniques\n",
    "\n",
    "## 5. \"Extra for experts\" ideas\n",
    "- Handle class imbalance (oversampling, undersampling, or SMOTE)\n",
    "- Experiment with different word embeddings (Word2Vec, GloVe, FastText) or contextual embeddings (ELMo, BERT)\n",
    "- Explore advanced model architectures (multi-head attention, capsule networks, memory-augmented networks)\n",
    "- Investigate transfer learning or multi-task learning\n",
    "- Conduct error analysis to understand and address specific issues\n",
    "- Develop a user interface or API for your sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4e9b2ba-e5d6-453c-aea1-d445c4249e61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import tokenizers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import PackedSequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class SimpleRNNCell(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.weight_ih = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
    "        self.weight_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.bias = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.weight_ih)\n",
    "        nn.init.xavier_uniform_(self.weight_hh)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        return F.tanh(\n",
    "            torch.matmul(input, self.weight_ih)\n",
    "            + torch.matmul(hidden, self.weight_hh)\n",
    "            + self.bias\n",
    "        )\n",
    "\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        num_layers: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.rnn_layers = nn.ModuleList(\n",
    "            [\n",
    "                SimpleRNNCell(input_size if layer == 0 else hidden_size, hidden_size)\n",
    "                for layer in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # type hints\n",
    "    def forward(\n",
    "        self,\n",
    "        input: torch.Tensor, # shape (L, B, C)\n",
    "        hidden: torch.Tensor = None,\n",
    "        lengths: torch.Tensor = None,\n",
    "    ):\n",
    "        if isinstance(input, PackedSequence):\n",
    "            input, lengths = pad_packed_sequence(input)\n",
    "        \n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(input.size(1))\n",
    "        \n",
    "        output = []\n",
    "        for input_t in input:\n",
    "            new_hidden = []\n",
    "            for layer, rnn_cell in enumerate(self.rnn_layers):\n",
    "                hidden[layer] = rnn_cell(input_t, hidden[layer])\n",
    "                new_hidden.append(hidden[layer])\n",
    "                input_t = hidden[layer]\n",
    "\n",
    "            output.append(input_t.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat(output, dim=0)\n",
    "\n",
    "        if lengths is not None:\n",
    "            output = pack_padded_sequence(output, lengths, enforce_sorted=False)\n",
    "\n",
    "        return output, torch.stack(new_hidden)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return [\n",
    "            torch.zeros(batch_size, self.hidden_size).to(\n",
    "                self.rnn_layers[0].weight_ih.device\n",
    "            )\n",
    "            for _ in range(self.num_layers)\n",
    "        ]\n",
    "\n",
    "\n",
    "class SentimentAnalysisModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        emb_dim: int = 30,\n",
    "        hidden_size: int = 40,\n",
    "        n_rnn_layers: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=emb_dim,\n",
    "        )\n",
    "        # self.rnn = nn.RNN(\n",
    "        #     input_size=emb_dim,\n",
    "        #     hidden_size=hidden_size,\n",
    "        #     num_layers=n_rnn_layers,\n",
    "        # )\n",
    "\n",
    "        # Using custom RNN\n",
    "        self.rnn = SimpleRNN(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_rnn_layers,\n",
    "        )\n",
    "\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 2),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor):\n",
    "        # x shape: (B, L)\n",
    "        # convert token indices to embedding values\n",
    "        x = self.emb(x)\n",
    "        # x shape: (B, L, Emb dim)\n",
    "        \n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        # # pack the sequence\n",
    "        x = pack_padded_sequence(x, lengths, enforce_sorted=False)\n",
    "\n",
    "        # # run the rnn, only taking the final rnn hidden state from the last layer\n",
    "        # # TODO: understand the difference between the two outputs more\n",
    "        _, x = self.rnn(x, lengths=lengths)\n",
    "        # x shape: (B, n_rnn_layers, Hidden size?)\n",
    "\n",
    "\n",
    "        # take only the last layer\n",
    "        x = x[-1, :, :]\n",
    "\n",
    "\n",
    "        # x shape: (B, Hidden size?)\n",
    "\n",
    "        return self.seq(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37b1b78a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in tokenized data\n",
    "data_dict = torch.load(\"data/imdb_data.pt\")\n",
    "data = data_dict[\"reviews\"]\n",
    "labels = data_dict[\"labels\"]\n",
    "lengths = data_dict[\"lengths\"]\n",
    "\n",
    "# split into train and test by 80:20\n",
    "training_fraction = 0.8\n",
    "\n",
    "train_data = data[: int(len(data) * training_fraction)]\n",
    "train_labels = labels[: int(len(data) * training_fraction)]\n",
    "train_lengths = lengths[: int(len(data) * training_fraction)]\n",
    "\n",
    "test_data = data[int(len(data) * training_fraction) :]\n",
    "test_labels = labels[int(len(data) * training_fraction) :]\n",
    "test_lengths = lengths[int(len(data) * training_fraction) :]\n",
    "\n",
    "\n",
    "# load in tokenizer\n",
    "tokenizer = tokenizers.Tokenizer.from_file(\"models/tokenizer.json\")\n",
    "vocab_size = tokenizer.get_vocab_size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885a232e",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6adca516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(\n",
    "    model: nn.Module,\n",
    "    train_data: torch.Tensor,\n",
    "    train_labels: torch.Tensor,\n",
    "    train_lengths: torch.Tensor,\n",
    "    batch_size: int,\n",
    "):\n",
    "    # calculate overall loss (need batching for memory reasons)\n",
    "    loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(train_data), batch_size):\n",
    "            training_data_batch = train_data[i : i + batch_size]\n",
    "            training_labels_batch = train_labels[i : i + batch_size]\n",
    "            training_lengths_batch = train_lengths[i : i + batch_size]\n",
    "\n",
    "            output = model(training_data_batch, lengths=training_lengths_batch)\n",
    "            ## Calculate correct predictions\n",
    "            _, y_predicted = torch.max(output, 1)\n",
    "            total += training_labels_batch.size(0)\n",
    "            correct += (y_predicted == training_labels_batch).sum().item()\n",
    "            loss += nn.functional.cross_entropy(output, training_labels_batch)\n",
    "\n",
    "    return loss, correct, total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb64173b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 2.09, tensor([ 0.2325, -0.2130,  0.2513], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch: 2, Loss: 2.08, tensor([ 0.2327, -0.2129,  0.2497], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch: 3, Loss: 2.08, tensor([ 0.2344, -0.2143,  0.2485], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch: 4, Loss: 2.09, tensor([ 0.2360, -0.2158,  0.2484], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "UPDATE - Epoch: 5, Train Acc: 49.93% (loss: 2.08), Test Acc:  50.00% (loss: 2.08)\n",
      "Epoch: 5, Loss: 2.08, tensor([ 0.2373, -0.2170,  0.2485], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch: 6, Loss: 2.08, tensor([ 0.2382, -0.2177,  0.2484], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch: 7, Loss: 2.08, tensor([ 0.2389, -0.2183,  0.2483], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch: 8, Loss: 2.08, tensor([ 0.2396, -0.2190,  0.2491], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch: 9, Loss: 2.08, tensor([ 0.2402, -0.2195,  0.2496], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "UPDATE - Epoch: 10, Train Acc: 50.65% (loss: 2.08), Test Acc:  49.22% (loss: 2.08)\n",
      "Epoch: 10, Loss: 2.08, tensor([ 0.2405, -0.2198,  0.2495], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch: 11, Loss: 2.08, tensor([ 0.2412, -0.2206,  0.2506], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch: 12, Loss: 2.08, tensor([ 0.2416, -0.2209,  0.2507], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch: 13, Loss: 2.08, tensor([ 0.2418, -0.2211,  0.2506], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch: 14, Loss: 2.08, tensor([ 0.2428, -0.2221,  0.2521], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "UPDATE - Epoch: 15, Train Acc: 51.24% (loss: 2.08), Test Acc:  50.26% (loss: 2.08)\n",
      "Epoch: 15, Loss: 2.08, tensor([ 0.2441, -0.2235,  0.2542], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch: 16, Loss: 2.08, tensor([ 0.2450, -0.2245,  0.2552], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch: 17, Loss: 2.08, tensor([ 0.2452, -0.2254,  0.2551], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch: 18, Loss: 2.08, tensor([ 0.2452, -0.2262,  0.2547], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch: 19, Loss: 2.09, tensor([ 0.2450, -0.2270,  0.2541], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "UPDATE - Epoch: 20, Train Acc: 49.61% (loss: 2.08), Test Acc:  49.80% (loss: 2.08)\n",
      "Epoch: 20, Loss: 2.08, tensor([ 0.2446, -0.2278,  0.2532], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch: 21, Loss: 2.08, tensor([ 0.2439, -0.2288,  0.2519], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch: 22, Loss: 2.08, tensor([ 0.2433, -0.2295,  0.2509], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch: 23, Loss: 2.08, tensor([ 0.2427, -0.2302,  0.2501], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch: 24, Loss: 2.08, tensor([ 0.2420, -0.2308,  0.2491], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "UPDATE - Epoch: 25, Train Acc: 49.09% (loss: 2.08), Test Acc:  51.43% (loss: 2.08)\n",
      "Epoch: 25, Loss: 2.08, tensor([ 0.2413, -0.2314,  0.2482], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch: 26, Loss: 2.08, tensor([ 0.2408, -0.2318,  0.2476], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Epoch: 27, Loss: 2.08, tensor([ 0.2407, -0.2320,  0.2474], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 80\u001b[0m\n\u001b[1;32m     77\u001b[0m training_lengths_batch \u001b[38;5;241m=\u001b[39m train_lengths[i : i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_lengths_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\u001b[39;00m\n\u001b[1;32m     83\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(output, training_labels_batch)\n",
      "File \u001b[0;32m~/source/pytorch_hello_worlds/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[16], line 142\u001b[0m, in \u001b[0;36mSentimentAnalysisModel.forward\u001b[0;34m(self, x, lengths)\u001b[0m\n\u001b[1;32m    138\u001b[0m x \u001b[38;5;241m=\u001b[39m pack_padded_sequence(x, lengths, enforce_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# # run the rnn, only taking the final rnn hidden state from the last layer\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# # TODO: understand the difference between the two outputs more\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m _, x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# x shape: (B, n_rnn_layers, Hidden size?)\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \n\u001b[1;32m    145\u001b[0m \n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# take only the last layer\u001b[39;00m\n\u001b[1;32m    147\u001b[0m x \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :, :]\n",
      "File \u001b[0;32m~/source/pytorch_hello_worlds/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[16], line 78\u001b[0m, in \u001b[0;36mSimpleRNN.forward\u001b[0;34m(self, input, hidden, lengths)\u001b[0m\n\u001b[1;32m     75\u001b[0m         new_hidden\u001b[38;5;241m.\u001b[39mappend(hidden[layer])\n\u001b[1;32m     76\u001b[0m         input_t \u001b[38;5;241m=\u001b[39m hidden[layer]\n\u001b[0;32m---> 78\u001b[0m     output\u001b[38;5;241m.\u001b[39mappend(\u001b[43minput_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     80\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lengths \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import json\n",
    "import time\n",
    "\n",
    "# nice file name including the date to store accuracy data\n",
    "date_str = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "batch_size = 512\n",
    "n_epochs = 100\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# create the model\n",
    "model = SentimentAnalysisModel(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_dim=30,\n",
    "    hidden_size=40,\n",
    "    n_rnn_layers=5,\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "subset = batch_size * 5\n",
    "\n",
    "train_data = train_data.to(device)[:subset]\n",
    "train_labels = train_labels.to(device)[:subset]\n",
    "train_lengths = train_lengths[:subset]\n",
    "\n",
    "test_data = test_data.to(device)[:subset]\n",
    "test_labels = test_labels.to(device)[:subset]\n",
    "test_lengths = test_lengths[:subset]\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "eval_interval = 5\n",
    "\n",
    "acc_best = 0\n",
    "acc_data = []\n",
    "for epoch in range(1, n_epochs):\n",
    "    if epoch % eval_interval == 0:\n",
    "        train_loss, train_correct, train_total = calculate_accuracy(\n",
    "            model, train_data, train_labels, train_lengths, batch_size\n",
    "        )\n",
    "        test_loss, test_correct, test_total = calculate_accuracy(\n",
    "            model, test_data, test_labels, test_lengths, batch_size\n",
    "        )\n",
    "        torch.save(model.state_dict(), f\"models/rnn_{date_str}_latest.pt\")\n",
    "\n",
    "        test_acc = test_correct / test_total * 100\n",
    "        train_acc = train_correct / train_total * 100\n",
    "\n",
    "        if test_acc > acc_best:\n",
    "            torch.save(model.state_dict(), f\"models/rnn_{date_str}_best.pt\")\n",
    "            acc_best = test_acc\n",
    "\n",
    "        print(\n",
    "            f\"UPDATE - Epoch: {epoch}, \"\n",
    "            f\"Train Acc: {train_acc:0.2f}% (loss: {train_loss:0.2f}), \"\n",
    "            f\"Test Acc: {test_acc: 0.2f}% (loss: {test_loss:0.2f})\"\n",
    "        )\n",
    "        acc_data.append(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"test_acc\": test_acc,\n",
    "                \"train_loss\": train_loss.item(),\n",
    "                \"test_loss\": test_loss.item(),\n",
    "            }\n",
    "        )\n",
    "        with open(f\"models/rnn_{date_str}_acc.json\", \"w\") as f:\n",
    "            json.dump(acc_data, f, indent=4)\n",
    "\n",
    "    cum_loss = 0\n",
    "\n",
    "    for i in range(0, len(train_data), batch_size):\n",
    "        training_data_batch = train_data[i : i + batch_size]\n",
    "        training_labels_batch = train_labels[i : i + batch_size]\n",
    "        training_lengths_batch = train_lengths[i : i + batch_size]\n",
    "\n",
    "        # forward pass\n",
    "        output = model(training_data_batch, lengths=training_lengths_batch)\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "\n",
    "        loss = nn.functional.cross_entropy(output, training_labels_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        cum_loss += loss.item()\n",
    "\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Loss: {cum_loss:0.2f}, {model.rnn.rnn_layers[0].weight_hh[1, :3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b83f8aac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4960, 0.5040],\n",
       "        [0.4955, 0.5045],\n",
       "        [0.4641, 0.5359],\n",
       "        ...,\n",
       "        [0.4846, 0.5154],\n",
       "        [0.4640, 0.5360],\n",
       "        [0.4741, 0.5259]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d721009",
   "metadata": {},
   "source": [
    "## Loading in latest model to check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03453a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the latest model\n",
    "loaded_model = SentimentAnalysisModel(vocab_size=vocab_size)\n",
    "loaded_model.load_state_dict(torch.load(\"models/rnn_latest.pt\"))\n",
    "loaded_model.to(device)\n",
    "\n",
    "test_data = test_data.to(device)\n",
    "test_labels = test_labels.to(device)\n",
    "\n",
    "test_stats = calculate_accuracy(\n",
    "    loaded_model, test_data, test_labels, test_lengths, batch_size\n",
    ")\n",
    "train_stats = calculate_accuracy(\n",
    "    loaded_model, train_data, train_labels, train_lengths, batch_size\n",
    ")\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"Test Loss: {test_stats[0]}, Test Accuracy: {test_stats[1]/test_stats[2]*100: .2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Train Loss: {train_stats[0]}, Train Accuracy: {train_stats[1]/train_stats[2]*100: .2f}\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
