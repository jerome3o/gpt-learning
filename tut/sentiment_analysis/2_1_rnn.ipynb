{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3a6cb3-d272-44ec-a666-ed634b9486f1",
   "metadata": {},
   "source": [
    "# Building a Deep Neural Net for Sentiment Analysis on IMDb Reviews\n",
    "\n",
    "## 1. Data collection and preprocessing\n",
    "- Collect a dataset of IMDb reviews\n",
    "- Preprocess the text data (tokenization, lowercasing, removing special characters, etc.)\n",
    "- Split the dataset into training, validation, and test sets\n",
    "\n",
    "## 2. **Model selection and architecture**\n",
    "- Research different types of deep learning models (**RNN**, LSTM, GRU, CNN, Transformer)\n",
    "- Decide on a model architecture\n",
    "- Experiment with pre-trained models (BERT, GPT, RoBERTa) for fine-tuning\n",
    "\n",
    "## 3. Model training and hyperparameter tuning\n",
    "- Set up a training loop\n",
    "- Use backpropagation to update the model's weights based on the loss function\n",
    "- Experiment with different hyperparameters (learning rate, batch size, dropout rate, etc.) and optimization algorithms (Adam, RMSprop, etc.)\n",
    "- Monitor performance on the validation set during training\n",
    "\n",
    "## 4. Model evaluation and refinement\n",
    "- Evaluate the model on the test set using relevant metrics (accuracy, F1 score, precision, recall, etc.)\n",
    "- Identify areas for improvement and iterate on the model architecture, training process, or preprocessing techniques\n",
    "\n",
    "## 5. \"Extra for experts\" ideas\n",
    "- Handle class imbalance (oversampling, undersampling, or SMOTE)\n",
    "- Experiment with different word embeddings (Word2Vec, GloVe, FastText) or contextual embeddings (ELMo, BERT)\n",
    "- Explore advanced model architectures (multi-head attention, capsule networks, memory-augmented networks)\n",
    "- Investigate transfer learning or multi-task learning\n",
    "- Conduct error analysis to understand and address specific issues\n",
    "- Develop a user interface or API for your sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4e9b2ba-e5d6-453c-aea1-d445c4249e61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import tokenizers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import PackedSequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class SimpleRNNCell(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.weight_ih = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
    "        self.weight_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.bias = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.weight_ih)\n",
    "        nn.init.xavier_uniform_(self.weight_hh)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        return F.tanh(\n",
    "            torch.matmul(input, self.weight_ih)\n",
    "            + torch.matmul(hidden, self.weight_hh)\n",
    "            + self.bias\n",
    "        )\n",
    "\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        num_layers: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.rnn_layers = nn.ModuleList(\n",
    "            [\n",
    "                SimpleRNNCell(input_size if layer == 0 else hidden_size, hidden_size)\n",
    "                for layer in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # type hints\n",
    "    def forward(\n",
    "        self,\n",
    "        input: torch.Tensor, # shape (L, B, C)\n",
    "        hidden: torch.Tensor = None,\n",
    "        lengths: torch.Tensor = None,\n",
    "    ):\n",
    "        if isinstance(input, PackedSequence):\n",
    "            input, lengths = pad_packed_sequence(input)\n",
    "        \n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(input.size(1))\n",
    "        \n",
    "        output = []\n",
    "        for input_t in input:\n",
    "            new_hidden = []\n",
    "            for layer, rnn_cell in enumerate(self.rnn_layers):\n",
    "                hidden[layer] = rnn_cell(input_t, hidden[layer])\n",
    "                new_hidden.append(hidden[layer])\n",
    "                input_t = hidden[layer]\n",
    "\n",
    "            output.append(input_t.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat(output, dim=0)\n",
    "\n",
    "        if lengths is not None:\n",
    "            output = pack_padded_sequence(output, lengths, enforce_sorted=False)\n",
    "\n",
    "        return output, torch.stack(new_hidden)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return [\n",
    "            torch.zeros(batch_size, self.hidden_size).to(\n",
    "                self.rnn_layers[0].weight_ih.device\n",
    "            )\n",
    "            for _ in range(self.num_layers)\n",
    "        ]\n",
    "\n",
    "\n",
    "class SentimentAnalysisModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        emb_dim: int = 30,\n",
    "        hidden_size: int = 40,\n",
    "        n_rnn_layers: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=emb_dim,\n",
    "        )\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_rnn_layers,\n",
    "        )\n",
    "\n",
    "        # # Using custom RNN\n",
    "        # self.rnn = SimpleRNN(\n",
    "        #     input_size=emb_dim,\n",
    "        #     hidden_size=hidden_size,\n",
    "        #     num_layers=n_rnn_layers,\n",
    "        # )\n",
    "\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor):\n",
    "        # x shape: (B, L)\n",
    "        # convert token indices to embedding values\n",
    "        x = self.emb(x)\n",
    "        # x shape: (B, L, Emb dim)\n",
    "        \n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        # # # # pack the sequence\n",
    "        # x = pack_padded_sequence(x, lengths, enforce_sorted=False)\n",
    "\n",
    "        # # run the rnn, only taking the final rnn hidden state from the last layer\n",
    "        # # TODO: understand the difference between the two outputs more\n",
    "        _, x = self.rnn(x) #, lengths=lengths)\n",
    "        # x shape: (B, n_rnn_layers, Hidden size?)\n",
    "\n",
    "\n",
    "        # take only the last layer\n",
    "        x = x[-1, :, :]\n",
    "\n",
    "\n",
    "        # x shape: (B, Hidden size?)\n",
    "\n",
    "        return self.seq(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37b1b78a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in tokenized data\n",
    "data_dict = torch.load(\"data/imdb_data.pt\")\n",
    "data = data_dict[\"reviews\"]\n",
    "labels = data_dict[\"labels\"]\n",
    "lengths = data_dict[\"lengths\"]\n",
    "\n",
    "# split into train and test by 80:20\n",
    "training_fraction = 0.8\n",
    "\n",
    "train_data = data[: int(len(data) * training_fraction)]\n",
    "train_labels = labels[: int(len(data) * training_fraction)]\n",
    "train_lengths = lengths[: int(len(data) * training_fraction)]\n",
    "\n",
    "test_data = data[int(len(data) * training_fraction) :]\n",
    "test_labels = labels[int(len(data) * training_fraction) :]\n",
    "test_lengths = lengths[int(len(data) * training_fraction) :]\n",
    "\n",
    "\n",
    "# load in tokenizer\n",
    "tokenizer = tokenizers.Tokenizer.from_file(\"models/tokenizer.json\")\n",
    "vocab_size = tokenizer.get_vocab_size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885a232e",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6adca516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(\n",
    "    model: nn.Module,\n",
    "    train_data: torch.Tensor,\n",
    "    train_labels: torch.Tensor,\n",
    "    train_lengths: torch.Tensor,\n",
    "    batch_size: int,\n",
    "):\n",
    "    # calculate overall loss (need batching for memory reasons)\n",
    "    loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(train_data), batch_size):\n",
    "            training_data_batch = train_data[i : i + batch_size]\n",
    "            training_labels_batch = train_labels[i : i + batch_size]\n",
    "            training_lengths_batch = train_lengths[i : i + batch_size]\n",
    "\n",
    "            output = model(training_data_batch, lengths=training_lengths_batch)\n",
    "            ## Calculate correct predictions\n",
    "            _, y_predicted = torch.max(output, 1)\n",
    "            total += training_labels_batch.size(0)\n",
    "            correct += (y_predicted == training_labels_batch).sum().item()\n",
    "            loss += nn.functional.cross_entropy(output, training_labels_batch)\n",
    "\n",
    "    return loss, correct, total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb64173b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 3.48\n",
      "Epoch: 2, Loss: 3.47\n",
      "Epoch: 3, Loss: 3.47\n",
      "Epoch: 4, Loss: 3.47\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import json\n",
    "import time\n",
    "\n",
    "# nice file name including the date to store accuracy data\n",
    "date_str = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "batch_size = 512\n",
    "n_epochs = 5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# create the model\n",
    "model = SentimentAnalysisModel(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_dim=30,\n",
    "    hidden_size=40,\n",
    "    n_rnn_layers=5,\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "subset = batch_size * 5\n",
    "\n",
    "train_data = train_data.to(device)[:subset]\n",
    "train_labels = train_labels.to(device)[:subset]\n",
    "train_lengths = train_lengths[:subset]\n",
    "\n",
    "test_data = test_data.to(device)[:subset]\n",
    "test_labels = test_labels.to(device)[:subset]\n",
    "test_lengths = test_lengths[:subset]\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "eval_interval = 5\n",
    "\n",
    "acc_best = 0\n",
    "acc_data = []\n",
    "for epoch in range(1, n_epochs):\n",
    "    if epoch % eval_interval == 0:\n",
    "        train_loss, train_correct, train_total = calculate_accuracy(\n",
    "            model, train_data, train_labels, train_lengths, batch_size\n",
    "        )\n",
    "        test_loss, test_correct, test_total = calculate_accuracy(\n",
    "            model, test_data, test_labels, test_lengths, batch_size\n",
    "        )\n",
    "        torch.save(model.state_dict(), f\"models/rnn_{date_str}_latest.pt\")\n",
    "\n",
    "        test_acc = test_correct / test_total * 100\n",
    "        train_acc = train_correct / train_total * 100\n",
    "\n",
    "        if test_acc > acc_best:\n",
    "            torch.save(model.state_dict(), f\"models/rnn_{date_str}_best.pt\")\n",
    "            acc_best = test_acc\n",
    "\n",
    "        print(\n",
    "            f\"UPDATE - Epoch: {epoch}, \"\n",
    "            f\"Train Acc: {train_acc:0.2f}% (loss: {train_loss:0.2f}), \"\n",
    "            f\"Test Acc: {test_acc: 0.2f}% (loss: {test_loss:0.2f})\"\n",
    "        )\n",
    "        acc_data.append(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"test_acc\": test_acc,\n",
    "                \"train_loss\": train_loss.item(),\n",
    "                \"test_loss\": test_loss.item(),\n",
    "            }\n",
    "        )\n",
    "        with open(f\"models/rnn_{date_str}_acc.json\", \"w\") as f:\n",
    "            json.dump(acc_data, f, indent=4)\n",
    "\n",
    "    cum_loss = 0\n",
    "\n",
    "    for i in range(0, len(train_data), batch_size):\n",
    "        training_data_batch = train_data[i : i + batch_size]\n",
    "        training_labels_batch = train_labels[i : i + batch_size]\n",
    "        training_lengths_batch = train_lengths[i : i + batch_size]\n",
    "\n",
    "        # forward pass\n",
    "        output = model(training_data_batch, lengths=training_lengths_batch)\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "\n",
    "        loss = nn.functional.cross_entropy(output, training_labels_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        cum_loss += loss.item()\n",
    "\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Loss: {cum_loss:0.2f}\") #, {model.rnn.rnn_layers[0].weight_hh[1, :3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b83f8aac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using a standard RNN from pytorch, it struggles to train without packing, \n",
    "# in the same way our custom RNN does!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d721009",
   "metadata": {},
   "source": [
    "## Loading in latest model to check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03453a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the latest model\n",
    "loaded_model = SentimentAnalysisModel(vocab_size=vocab_size)\n",
    "loaded_model.load_state_dict(torch.load(\"models/rnn_latest.pt\"))\n",
    "loaded_model.to(device)\n",
    "\n",
    "test_data = test_data.to(device)\n",
    "test_labels = test_labels.to(device)\n",
    "\n",
    "test_stats = calculate_accuracy(\n",
    "    loaded_model, test_data, test_labels, test_lengths, batch_size\n",
    ")\n",
    "train_stats = calculate_accuracy(\n",
    "    loaded_model, train_data, train_labels, train_lengths, batch_size\n",
    ")\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"Test Loss: {test_stats[0]}, Test Accuracy: {test_stats[1]/test_stats[2]*100: .2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Train Loss: {train_stats[0]}, Train Accuracy: {train_stats[1]/train_stats[2]*100: .2f}\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
