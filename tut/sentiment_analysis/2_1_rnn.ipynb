{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3a6cb3-d272-44ec-a666-ed634b9486f1",
   "metadata": {},
   "source": [
    "# Building a Deep Neural Net for Sentiment Analysis on IMDb Reviews\n",
    "\n",
    "## 1. Data collection and preprocessing\n",
    "- Collect a dataset of IMDb reviews\n",
    "- Preprocess the text data (tokenization, lowercasing, removing special characters, etc.)\n",
    "- Split the dataset into training, validation, and test sets\n",
    "\n",
    "## 2. **Model selection and architecture**\n",
    "- Research different types of deep learning models (**RNN**, LSTM, GRU, CNN, Transformer)\n",
    "- Decide on a model architecture\n",
    "- Experiment with pre-trained models (BERT, GPT, RoBERTa) for fine-tuning\n",
    "\n",
    "## 3. Model training and hyperparameter tuning\n",
    "- Set up a training loop\n",
    "- Use backpropagation to update the model's weights based on the loss function\n",
    "- Experiment with different hyperparameters (learning rate, batch size, dropout rate, etc.) and optimization algorithms (Adam, RMSprop, etc.)\n",
    "- Monitor performance on the validation set during training\n",
    "\n",
    "## 4. Model evaluation and refinement\n",
    "- Evaluate the model on the test set using relevant metrics (accuracy, F1 score, precision, recall, etc.)\n",
    "- Identify areas for improvement and iterate on the model architecture, training process, or preprocessing techniques\n",
    "\n",
    "## 5. \"Extra for experts\" ideas\n",
    "- Handle class imbalance (oversampling, undersampling, or SMOTE)\n",
    "- Experiment with different word embeddings (Word2Vec, GloVe, FastText) or contextual embeddings (ELMo, BERT)\n",
    "- Explore advanced model architectures (multi-head attention, capsule networks, memory-augmented networks)\n",
    "- Investigate transfer learning or multi-task learning\n",
    "- Conduct error analysis to understand and address specific issues\n",
    "- Develop a user interface or API for your sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4e9b2ba-e5d6-453c-aea1-d445c4249e61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import tokenizers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "class JeromeRNNInnerModule(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        activation_function=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._activation = activation_function or F.tanh\n",
    "        self._input_size = input_size\n",
    "        self._hidden_size = hidden_size\n",
    "\n",
    "        # use Xavier initialisation to avoid exploding exponentials\n",
    "        self._w_ax = nn.Parameter(torch.Tensor(self._input_size + self._hidden_size, self._hidden_size))\n",
    "        nn.init.kaiming_uniform_(self._w_ax)\n",
    "\n",
    "        # bias\n",
    "        self._b_ax = nn.Parameter(torch.zeros(self._hidden_size))\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        # single token\n",
    "        # x shape is (L, B, _input_size)\n",
    "\n",
    "        L, B, _ = x.size()\n",
    "        output = torch.zeros((L, B, self._hidden_size), device=x.device)\n",
    "\n",
    "        for t in range(L):\n",
    "            x_t = x[t]\n",
    "            a = self._activation(\n",
    "                torch.matmul(\n",
    "                    torch.concat((x_t, a), dim=-1),\n",
    "                    self._w_ax\n",
    "                ) + self._b_ax\n",
    "            )\n",
    "            output[t, :, :] = a\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class JeromeRNNBlockModule(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        n_layers: int,\n",
    "        activation_function=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._hidden_size = hidden_size\n",
    "        self._input_size = input_size\n",
    "        self._layers = nn.ModuleList([\n",
    "            JeromeRNNInnerModule(\n",
    "                input_size=input_size if i == 0 else hidden_size,\n",
    "                hidden_size=hidden_size,\n",
    "                activation_function=activation_function,\n",
    "            )\n",
    "            for i in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape: (L, B, _input_size)\n",
    "        _, batch_size, _ = x.size()\n",
    "\n",
    "        for i, layer in enumerate(self._layers):\n",
    "            a = torch.zeros((batch_size, self._hidden_size), device=x.device)\n",
    "            x = layer(x, a)\n",
    "        \n",
    "        return x[-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SentimentAnalysisModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        emb_dim: int = 30,\n",
    "        hidden_size: int = 40,\n",
    "        n_rnn_layers: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=emb_dim,\n",
    "        )\n",
    "        # self.rnn = nn.RNN(\n",
    "        #     input_size=emb_dim,\n",
    "        #     hidden_size=hidden_size,\n",
    "        #     num_layers=n_rnn_layers,\n",
    "        # )\n",
    "\n",
    "        # Using Jerome's RNN\n",
    "        self.rnn = JeromeRNNBlockModule(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            n_layers=n_rnn_layers,\n",
    "        )\n",
    "\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 2),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor):\n",
    "        # x shape: (B, L)\n",
    "        # convert token indices to embedding values\n",
    "        x = self.emb(x)\n",
    "        # x shape: (B, L, Emb dim)\n",
    "\n",
    "        x = x.transpose(0, 1)\n",
    "        # x shape: (L, B, Emb dim)\n",
    "\n",
    "        # # pack the sequence\n",
    "        # x = pack_padded_sequence(x, lengths, enforce_sorted=False)\n",
    "\n",
    "        # # run the rnn, only taking the final rnn hidden state from the last layer\n",
    "        # # TODO: understand the difference between the two outputs more\n",
    "        # _, x = self.rnn(x)\n",
    "        # # x shape: (B, n_rnn_layers, Hidden size?)\n",
    "        \n",
    "        # # take only the last layer\n",
    "        # x = x[-1, :, :]\n",
    "\n",
    "        # Using Jerome's RNN\n",
    "        x = self.rnn(x)\n",
    "\n",
    "        # x shape: (B, Hidden size?)\n",
    "        \n",
    "        return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37b1b78a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in tokenized data\n",
    "data_dict = torch.load(\"data/imdb_data.pt\")\n",
    "data = data_dict[\"reviews\"]\n",
    "labels = data_dict[\"labels\"]\n",
    "lengths = data_dict[\"lengths\"]\n",
    "\n",
    "# split into train and test by 80:20\n",
    "training_fraction = 0.8\n",
    "\n",
    "train_data = data[:int(len(data) * training_fraction)]\n",
    "train_labels = labels[:int(len(data) * training_fraction)]\n",
    "train_lengths = lengths[:int(len(data) * training_fraction)]\n",
    "\n",
    "test_data = data[int(len(data) * training_fraction):]\n",
    "test_labels = labels[int(len(data) * training_fraction):]\n",
    "test_lengths = lengths[int(len(data) * training_fraction):]\n",
    "\n",
    "\n",
    "# load in tokenizer\n",
    "tokenizer = tokenizers.Tokenizer.from_file(\"models/tokenizer.json\")\n",
    "vocab_size = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885a232e",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6adca516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(\n",
    "    model: nn.Module,\n",
    "    train_data: torch.Tensor,\n",
    "    train_labels: torch.Tensor,\n",
    "    train_lengths: torch.Tensor,\n",
    "    batch_size: int,\n",
    "):\n",
    "    # calculate overall loss (need batching for memory reasons)\n",
    "    loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(train_data), batch_size):\n",
    "            training_data_batch = train_data[i : i + batch_size]\n",
    "            training_labels_batch = train_labels[i : i + batch_size]\n",
    "            training_lengths_batch = train_lengths[i : i + batch_size]\n",
    "\n",
    "            output = model(training_data_batch, lengths=training_lengths_batch)\n",
    "            ## Calculate correct predictions\n",
    "            _, y_predicted = torch.max(output, 1)\n",
    "            total += training_labels_batch.size(0)\n",
    "            correct += (y_predicted == training_labels_batch).sum().item()\n",
    "            loss += nn.functional.cross_entropy(output, training_labels_batch)\n",
    "\n",
    "    return loss, correct, total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb64173b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATE - Epoch: 0, Train Acc: 49.09% (loss: 2.08), Test Acc:  49.61% (loss: 2.08)\n",
      "Epoch: 0, Loss: 2.37\n",
      "UPDATE - Epoch: 1, Train Acc: 49.09% (loss: 2.47), Test Acc:  49.61% (loss: 2.45)\n",
      "Epoch: 1, Loss: nan\n",
      "UPDATE - Epoch: 2, Train Acc: 49.09% (loss: nan), Test Acc:  49.61% (loss: nan)\n",
      "Epoch: 2, Loss: nan\n",
      "UPDATE - Epoch: 3, Train Acc: 49.09% (loss: nan), Test Acc:  49.61% (loss: nan)\n",
      "Epoch: 3, Loss: nan\n",
      "UPDATE - Epoch: 4, Train Acc: 49.09% (loss: nan), Test Acc:  49.61% (loss: nan)\n",
      "Epoch: 4, Loss: nan\n",
      "UPDATE - Epoch: 5, Train Acc: 49.09% (loss: nan), Test Acc:  49.61% (loss: nan)\n",
      "Epoch: 5, Loss: nan\n",
      "UPDATE - Epoch: 6, Train Acc: 49.09% (loss: nan), Test Acc:  49.61% (loss: nan)\n",
      "Epoch: 6, Loss: nan\n",
      "UPDATE - Epoch: 7, Train Acc: 49.09% (loss: nan), Test Acc:  49.61% (loss: nan)\n",
      "Epoch: 7, Loss: nan\n",
      "UPDATE - Epoch: 8, Train Acc: 49.09% (loss: nan), Test Acc:  49.61% (loss: nan)\n",
      "Epoch: 8, Loss: nan\n",
      "UPDATE - Epoch: 9, Train Acc: 49.09% (loss: nan), Test Acc:  49.61% (loss: nan)\n",
      "Epoch: 9, Loss: nan\n",
      "UPDATE - Epoch: 10, Train Acc: 49.09% (loss: nan), Test Acc:  49.61% (loss: nan)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m output \u001b[38;5;241m=\u001b[39m model(training_data_batch, lengths\u001b[38;5;241m=\u001b[39mtraining_lengths_batch)\n\u001b[1;32m     84\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(output, training_labels_batch)\n\u001b[0;32m---> 85\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# During training, after loss.backward()\u001b[39;00m\n\u001b[1;32m     87\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/source/pytorch_hello_worlds/venv/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/source/pytorch_hello_worlds/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:204\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import json\n",
    "import time\n",
    "\n",
    "# nice file name including the date to store accuracy data\n",
    "date_str = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# create the model\n",
    "model = SentimentAnalysisModel(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_dim=30,\n",
    "    hidden_size=40,\n",
    "    n_rnn_layers=3,\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "subset = batch_size * 3\n",
    "\n",
    "train_data = train_data.to(device)[:subset]\n",
    "train_labels = train_labels.to(device)[:subset]\n",
    "train_lengths = train_lengths.to(device)[:subset]\n",
    "\n",
    "test_data = test_data.to(device)[:subset]\n",
    "test_labels = test_labels.to(device)[:subset]\n",
    "test_lengths = test_lengths.to(device)[:subset]\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "eval_interval = 1\n",
    "\n",
    "acc_best = 0\n",
    "acc_data = []\n",
    "\n",
    "# train the model\n",
    "for epoch in range(500):\n",
    "    if epoch % eval_interval == 0:\n",
    "        train_loss, train_correct, train_total = calculate_accuracy(\n",
    "            model, train_data, train_labels, train_lengths, batch_size\n",
    "        )\n",
    "        test_loss, test_correct, test_total = calculate_accuracy(\n",
    "            model, test_data, test_labels, test_lengths, batch_size\n",
    "        )\n",
    "        torch.save(model.state_dict(), f\"models/rnn_{date_str}_latest.pt\")\n",
    "\n",
    "        test_acc = test_correct / test_total * 100\n",
    "        train_acc = train_correct / train_total * 100\n",
    "\n",
    "        if test_acc > acc_best:\n",
    "            torch.save(model.state_dict(), f\"models/rnn_{date_str}_best.pt\")\n",
    "            acc_best = test_acc\n",
    "\n",
    "        print(\n",
    "            f\"UPDATE - Epoch: {epoch}, \"\n",
    "            f\"Train Acc: {train_acc:0.2f}% (loss: {train_loss:0.2f}), \"\n",
    "            f\"Test Acc: {test_acc: 0.2f}% (loss: {test_loss:0.2f})\"\n",
    "        )\n",
    "        acc_data.append(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"test_acc\": test_acc,\n",
    "                \"train_loss\": train_loss.item(),\n",
    "                \"test_loss\": test_loss.item(),\n",
    "            }\n",
    "        )\n",
    "        with open(f\"models/rnn_{date_str}_acc.json\", \"w\") as f:\n",
    "            json.dump(acc_data, f, indent=4)\n",
    "\n",
    "    cum_loss = 0\n",
    "\n",
    "    for i in range(0, len(train_data), batch_size):\n",
    "        training_data_batch = train_data[i : i + batch_size]\n",
    "        training_labels_batch = train_labels[i : i + batch_size]\n",
    "        training_lengths_batch = train_lengths[i : i + batch_size]\n",
    "\n",
    "        # forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(training_data_batch, lengths=training_lengths_batch)\n",
    "\n",
    "        loss = nn.functional.cross_entropy(output, training_labels_batch)\n",
    "        loss.backward()\n",
    "        # During training, after loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        optimizer.step()\n",
    "        cum_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Loss: {cum_loss:0.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83f8aac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d721009",
   "metadata": {},
   "source": [
    "## Loading in latest model to check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03453a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the latest model\n",
    "loaded_model = SentimentAnalysisModel(vocab_size=vocab_size)\n",
    "loaded_model.load_state_dict(torch.load(\"models/rnn_latest.pt\"))\n",
    "loaded_model.to(device)\n",
    "\n",
    "test_data = test_data.to(device)\n",
    "test_labels = test_labels.to(device)\n",
    "\n",
    "test_stats = calculate_accuracy(loaded_model, test_data, test_labels, test_lengths, batch_size)\n",
    "train_stats = calculate_accuracy(loaded_model, train_data, train_labels, train_lengths, batch_size)\n",
    "\n",
    "\n",
    "print(f\"Test Loss: {test_stats[0]}, Test Accuracy: {test_stats[1]/test_stats[2]*100: .2f}\")\n",
    "print(f\"Train Loss: {train_stats[0]}, Train Accuracy: {train_stats[1]/train_stats[2]*100: .2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
