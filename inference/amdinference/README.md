# AMD Inference Server

This is me following along with this [quickstart guide](https://xilinx.github.io/inference-server/main/quickstart.html)

## GPU

For running models on the GPU, it [looks like we need to have the model in ONNX format](https://xilinx.github.io/inference-server/main/introduction.html#features).

This these resources might be useful:
    * [PyTorch guide 1](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html)
    * [PyTorch guide 2](https://pytorch.org/docs/stable/onnx.html)
    * [MarkLogic guide](https://docs.marklogic.com/guide/app-dev/PyTorch)
